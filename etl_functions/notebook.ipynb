{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ú® Descripci√≥n General del Sistema ‚ú®\n",
    "\n",
    "Este sistema est√° dise√±ado para **procesar**, **analizar** y **visualizar datos** relacionados con incidentes delictivos en diferentes distritos, complementando esta informaci√≥n con estad√≠sticas socioecon√≥micas. A trav√©s de un flujo organizado, se busca descubrir patrones y generar insights valiosos de manera eficiente.\n",
    "\n",
    "## üéØ **Objetivos del Sistema**\n",
    "\n",
    "1. üõ†Ô∏è **Preprocesamiento de Datos**:\n",
    "   - ‚úÖ Limpieza y normalizaci√≥n de datos, eliminando inconsistencias como tildes y caracteres no deseados.\n",
    "   - ‚úÖ Estandarizaci√≥n de formatos para facilitar comparaciones entre m√∫ltiples conjuntos de datos.\n",
    "   - ‚úÖ Conversi√≥n de columnas, como fechas y horas, para asegurar su correcto manejo.\n",
    "\n",
    "2. üìä **An√°lisis de Datos**:\n",
    "   - üîç Identificar patrones, correlaciones y tendencias en los datos.\n",
    "   - üó∫Ô∏è Filtrar y agrupar informaci√≥n por categor√≠as clave (distritos, horas, tipos de delitos, etc.).\n",
    "\n",
    "3. üé® **Visualizaci√≥n de Resultados**:\n",
    "   - üìâ Creaci√≥n de gr√°ficos claros y din√°micos para representar los resultados.\n",
    "   - üå°Ô∏è Generaci√≥n de mapas de calor, gr√°ficos de barras y otras visualizaciones atractivas.\n",
    "   - üìÖ An√°lisis temporal (d√≠as de la semana, horas cr√≠ticas, etc.).\n",
    "\n",
    "## üèóÔ∏è **Estructura del Sistema**\n",
    "\n",
    "### 1Ô∏è‚É£ **Preprocesamiento de Datos**\n",
    "- üßπ **Limpieza**: Eliminaci√≥n de inconsistencias como tildes, espacios extra y caracteres especiales.\n",
    "- üîÑ **Transformaci√≥n**: Conversi√≥n de cadenas a fechas, horas, y otros formatos requeridos.\n",
    "- ‚ûï **Enriquecimiento**: Generaci√≥n de nuevas columnas derivadas, como d√≠as de la semana y rangos horarios.\n",
    "\n",
    "### 2Ô∏è‚É£ **Visualizaci√≥n de los Datos**\n",
    "- üìä **Gr√°ficos por Categor√≠a**:\n",
    "  - An√°lisis de delitos por tipo, sexo, y hora del d√≠a.\n",
    "  - Relaci√≥n entre factores socioecon√≥micos y la cantidad de delitos.\n",
    "- üå°Ô∏è **Mapas de Calor**:\n",
    "  - Representaci√≥n de delitos por hora y tipo, destacando patrones cr√≠ticos.\n",
    "- üèÜ **Insights Visuales**:\n",
    "  - Explorar c√≥mo factores como la tasa de ocupaci√≥n influyen en la cantidad de delitos.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö° **Beneficios del Sistema**:\n",
    "- üìà **Escalabilidad**: Puede manejar grandes conjuntos de datos.\n",
    "- üõ†Ô∏è **Flexibilidad**: Adaptable a diferentes estructuras y fuentes de datos.\n",
    "- üí° **Insights Accionables**: Facilita la toma de decisiones informadas basadas en datos reales.\n",
    "\n",
    "‚ú® ¬°Transforma datos en conocimiento con este sistema! ‚ú®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexi√≥n Base de Datos Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "import os\n",
    "\n",
    "# ================================================\n",
    "# Definici√≥n de par√°metros de conexi√≥n\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque define los par√°metros necesarios para conectarse a una base de datos PostgreSQL.\n",
    "# Los par√°metros incluyen:\n",
    "# - `dbname`: Nombre de la base de datos a la que se conectar√°.\n",
    "# - `user`: Usuario de la base de datos.\n",
    "# - `password`: Contrase√±a para autenticar al usuario.\n",
    "# - `host`: Direcci√≥n del host donde se encuentra la base de datos.\n",
    "# - `port`: Puerto en el que escucha la base de datos.\n",
    "\n",
    "conn_params = {\n",
    "    'dbname': 'datos',        # Nombre de la base de datos\n",
    "    'user': 'postgres',       # Usuario de la base de datos\n",
    "    'password': '12345',      # Contrase√±a del usuario\n",
    "    'host': 'localhost',      # Direcci√≥n del servidor (localhost en este caso)\n",
    "    'port': '5432'            # Puerto por defecto de PostgreSQL\n",
    "}\n",
    "\n",
    "# ================================================\n",
    "# Establecer conexi√≥n con la base de datos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque intenta establecer una conexi√≥n con la base de datos PostgreSQL\n",
    "# utilizando los par√°metros definidos anteriormente.\n",
    "# En caso de √©xito, imprime un mensaje de confirmaci√≥n.\n",
    "# Si ocurre un error, este se captura y se imprime en la consola.\n",
    "\n",
    "try:\n",
    "    # Se intenta conectar a la base de datos utilizando los par√°metros\n",
    "    conn = psycopg.connect(**conn_params)\n",
    "    print(\"Conexi√≥n exitosa\")\n",
    "except Exception as e:\n",
    "    # Captura cualquier error durante la conexi√≥n y lo imprime\n",
    "    print(f\"Ocurri√≥ un error: {e}\")\n",
    "\n",
    "# ================================================\n",
    "# Cierre de conexi√≥n (comentado por defecto)\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Es importante cerrar la conexi√≥n a la base de datos una vez que se han\n",
    "# completado todas las operaciones. Este paso evita fugas de recursos.\n",
    "\n",
    "# Descomenta la siguiente l√≠nea para cerrar la conexi√≥n cuando termines.\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Funci√≥n que elimine los espacios en blanco de la columna distrito para usarse en ambos conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# ================================================\n",
    "# Crea una sesi√≥n de Spark\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque crea una sesi√≥n de Spark, que es necesaria para ejecutar comandos de PySpark.\n",
    "# La sesi√≥n de Spark permite procesar datos a gran escala utilizando DataFrames y RDDs.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Preprocessing Data\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# ================================================\n",
    "# Carga de datos desde archivos CSV\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Los datos se cargan desde dos archivos CSV: uno contiene datos del OIJ y el otro del INEC.\n",
    "# Ambos DataFrames se construyen especificando encabezados y permitiendo la inferencia de tipos de datos.\n",
    "\n",
    "# Par√°metros de entrada:\n",
    "# - Ruta del archivo: La ruta absoluta de los archivos CSV.\n",
    "# - header=True: Especifica que los archivos CSV tienen encabezados.\n",
    "# - inferSchema=True: Permite que Spark detecte autom√°ticamente los tipos de datos.\n",
    "\n",
    "# Descripci√≥n de la salida:\n",
    "# - `oij_df` y `inec_df`: DataFrames de Spark que contienen los datos cargados.\n",
    "oij_df = spark.read.csv(\n",
    "    \"C:\\\\Users\\\\grana\\\\OneDrive\\\\Escritorio\\\\Bases de datos\\\\spark\\\\data\\\\OIJ.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "inec_df = spark.read.csv(\n",
    "    \"C:\\\\Users\\\\grana\\\\OneDrive\\\\Escritorio\\\\Bases de datos\\\\spark\\\\data\\\\inec.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: eliminar_espacios_y_concatenar\n",
    "# ================================================\n",
    "def eliminar_espacios_y_concatenar(dataframe, columna):\n",
    "    \"\"\"\n",
    "    Elimina los espacios en blanco y concatena las palabras en una columna espec√≠fica \n",
    "    de un DataFrame de Spark.\n",
    "\n",
    "    Par√°metros:\n",
    "    - dataframe (DataFrame): El DataFrame de Spark a procesar.\n",
    "    - columna (str): El nombre de la columna en la que se eliminar√°n los espacios en blanco.\n",
    "\n",
    "    Retorno:\n",
    "    - DataFrame: Un nuevo DataFrame con los cambios aplicados a la columna especificada.\n",
    "\n",
    "    Excepciones:\n",
    "    - ValueError: Si la columna especificada no existe en el DataFrame.\n",
    "\n",
    "    Descripci√≥n de bloques relevantes:\n",
    "    - El bloque `if columna in dataframe.columns` verifica si la columna existe.\n",
    "    - La funci√≥n `regexp_replace` se utiliza para eliminar los espacios en blanco.\n",
    "    \"\"\"\n",
    "    if columna in dataframe.columns:\n",
    "        return dataframe.withColumn(columna, regexp_replace(dataframe[columna], \" \", \"\"))\n",
    "    else:\n",
    "        raise ValueError(f\"La columna '{columna}' no existe en el DataFrame.\")\n",
    "\n",
    "# ================================================\n",
    "# Limpieza del DataFrame de OIJ\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque limpia la columna 'Distrito' en el DataFrame `oij_df`.\n",
    "# Tambi√©n elimina la columna '_C11' si est√° presente.\n",
    "\n",
    "try:\n",
    "    # Limpia la columna 'Distrito' eliminando espacios en blanco\n",
    "    oij_df = eliminar_espacios_y_concatenar(oij_df, \"Provincia\")\n",
    "    oij_df = eliminar_espacios_y_concatenar(oij_df, \"Canton\")\n",
    "    oij_df = eliminar_espacios_y_concatenar(oij_df, 'Distrito')\n",
    "\n",
    "    \n",
    "    # Elimina la columna '_C11' si existe\n",
    "    if '_C11' in oij_df.columns:\n",
    "        oij_df = oij_df.drop('_C11')\n",
    "except Exception as e:\n",
    "    # Captura errores durante el procesamiento\n",
    "    print(f\"Error al procesar OIJ: {e}\")\n",
    "\n",
    "# ================================================\n",
    "# Limpieza del DataFrame de INEC\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque limpia la columna 'distrito' en el DataFrame `inec_df`.\n",
    "\n",
    "try:\n",
    "    # Limpia la columna 'distrito' eliminando espacios en blanco\n",
    "    inec_df = eliminar_espacios_y_concatenar(inec_df, \"Provincia\")\n",
    "    inec_df = eliminar_espacios_y_concatenar(inec_df, \"Canton\")\n",
    "    inec_df = eliminar_espacios_y_concatenar(inec_df, \"distrito\")\n",
    "except Exception as e:\n",
    "    # Captura errores durante el procesamiento\n",
    "    print(f\"Error al procesar INEC: {e}\")\n",
    "\n",
    "# ================================================\n",
    "# Visualizaci√≥n de los resultados\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque muestra los primeros 50 registros de cada DataFrame despu√©s del preprocesamiento.\n",
    "\n",
    "# Muestra el DataFrame de OIJ despu√©s de la limpieza\n",
    "print(\"Datos del OIJ despu√©s de limpiar:\")\n",
    "oij_df.show(50, truncate=False)\n",
    "\n",
    "# Muestra el DataFrame de INEC despu√©s de la limpieza\n",
    "print(\"Datos del INEC despu√©s de limpiar:\")\n",
    "inec_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Funci√≥n que convierta a min√∫sculas el contenido de la columna distrito para usarse en ambos conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# ================================================\n",
    "# Crear SparkSession\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque inicializa una SparkSession, que es el punto de entrada para usar PySpark.\n",
    "# Adem√°s, se configura el paquete de conexi√≥n para PostgreSQL.\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Proyecto SparkSQL\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\") \\\n",
    "                .getOrCreate()\n",
    "    print(\"SparkSession creada:\", spark.version)  # Imprime la versi√≥n de Spark en uso\n",
    "except Exception as e:\n",
    "    # Captura y muestra errores en caso de fallos al crear la sesi√≥n\n",
    "    print(\"Error al crear SparkSession:\", str(e))\n",
    "\n",
    "# ================================================\n",
    "# Cargar CSVs\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque carga datos desde archivos CSV en DataFrames de Spark.\n",
    "# Actualmente, este bloque est√° comentado para evitar errores si los archivos no est√°n disponibles.\n",
    "\n",
    "# df_oij = spark.read.csv(\"C:\\\\Projects\\\\spark\\\\data\\\\OIJ2011.csv\", header=True, inferSchema=True)\n",
    "# df_inec = spark.read.csv(\"C:\\\\Projects\\\\spark\\\\data\\\\inec.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: convertir_minusculas\n",
    "# ================================================\n",
    "def convertir_minusculas(df, columna):\n",
    "    \"\"\"\n",
    "    Convierte los valores de una columna espec√≠fica a min√∫sculas en un DataFrame de Spark.\n",
    "    \n",
    "    Par√°metros:\n",
    "    - df (DataFrame): DataFrame de Spark que contiene los datos.\n",
    "    - columna (str): Nombre de la columna cuyos valores se transformar√°n a min√∫sculas.\n",
    "    \n",
    "    Retorno:\n",
    "    - DataFrame: Un nuevo DataFrame con la columna especificada transformada a min√∫sculas.\n",
    "    \n",
    "    Descripci√≥n de bloques relevantes:\n",
    "    - Se utiliza la funci√≥n `lower` de PySpark para convertir los valores.\n",
    "    - `withColumn` crea una nueva versi√≥n de la columna especificada con los valores modificados.\n",
    "    \"\"\"\n",
    "    return df.withColumn(columna, lower(df[columna]))\n",
    "\n",
    "# ================================================\n",
    "# Aplicar la transformaci√≥n a min√∫sculas\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Aplica la funci√≥n `convertir_minusculas` a columnas espec√≠ficas de ambos DataFrames.\n",
    "# Esto asegura que los valores tengan el mismo formato para facilitar an√°lisis posteriores.\n",
    "\n",
    "try:\n",
    "    # Transformar la columna 'Distrito' en df_oij\n",
    "    \n",
    "    df_oij = convertir_minusculas(oij_df, \"Provincia\")\n",
    "    df_oij = convertir_minusculas(df_oij, \"Canton\")\n",
    "    df_oij = convertir_minusculas(df_oij, \"Distrito\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Transformar la columna 'distrito' en df_inec\n",
    "    df_inec = convertir_minusculas(inec_df, \"Provincia\")\n",
    "    df_inec = convertir_minusculas(df_inec, \"Canton\")\n",
    "    df_inec = convertir_minusculas(df_inec, \"distrito\")\n",
    "   \n",
    "   \n",
    "except Exception as e:\n",
    "    # Captura cualquier error relacionado con la transformaci√≥n\n",
    "    print(\"Error al aplicar la transformaci√≥n:\", str(e))\n",
    "\n",
    "# ================================================\n",
    "# Mostrar los resultados\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque imprime los resultados de los DataFrames despu√©s de aplicar la transformaci√≥n.\n",
    "# Se utiliza `show()` para mostrar los datos en la consola.\n",
    "\n",
    "print(\"Datos OIJ:\")\n",
    "df_oij.show(truncate=False)  # Muestra los datos del DataFrame df_oij\n",
    "\n",
    "print(\"Datos INEC:\")\n",
    "df_inec.show(truncate=False)  # Muestra los datos del DataFrame df_inec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear Tablas para Ambos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, BooleanType\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: crear_tabla_sql\n",
    "# ================================================\n",
    "def crear_tabla_sql(df, nombre_tabla):\n",
    "    \"\"\"\n",
    "    Genera el script SQL para crear una tabla en PostgreSQL a partir de un DataFrame de Spark.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de Spark cuyo esquema ser√° usado para definir la tabla.\n",
    "        nombre_tabla: Nombre de la tabla a crear en PostgreSQL.\n",
    "    \n",
    "    Returns:\n",
    "        str: Script SQL para crear la tabla con columnas correspondientes a los tipos del DataFrame.\n",
    "    \"\"\"\n",
    "    schema = df.schema  # Obtiene el esquema del DataFrame\n",
    "    columnas = []       # Lista para almacenar definiciones de columnas\n",
    "    for campo in schema.fields:\n",
    "        # Mapea los tipos de datos de Spark a los tipos de PostgreSQL\n",
    "        if isinstance(campo.dataType, StringType):\n",
    "            tipo_postgres = \"VARCHAR\"\n",
    "        elif isinstance(campo.dataType, IntegerType):\n",
    "            tipo_postgres = \"INTEGER\"\n",
    "        elif isinstance(campo.dataType, FloatType):\n",
    "            tipo_postgres = \"FLOAT\"\n",
    "        elif isinstance(campo.dataType, DoubleType):\n",
    "            tipo_postgres = \"DOUBLE PRECISION\"\n",
    "        elif isinstance(campo.dataType, BooleanType):\n",
    "            tipo_postgres = \"BOOLEAN\"\n",
    "        else:\n",
    "            tipo_postgres = \"TEXT\"  # Tipo por defecto\n",
    "        columnas.append(f\"{campo.name} {tipo_postgres}\")\n",
    "    \n",
    "    # Construye la consulta SQL\n",
    "    columnas_str = \", \".join(columnas)\n",
    "    return f\"CREATE TABLE IF NOT EXISTS {nombre_tabla} ({columnas_str});\"\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: insertar_datos_postgres\n",
    "# ================================================\n",
    "def insertar_datos_postgres(df, nombre_tabla, conn_params):\n",
    "    \"\"\"\n",
    "    Inserta los datos de un DataFrame de Spark en una tabla PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de Spark cuyos datos ser√°n insertados.\n",
    "        nombre_tabla: Nombre de la tabla en PostgreSQL donde se insertar√°n los datos.\n",
    "        conn_params: Par√°metros de conexi√≥n a PostgreSQL como diccionario.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Excepciones:\n",
    "        Muestra un mensaje en caso de error durante la inserci√≥n.\n",
    "    \"\"\"\n",
    "    # Convierte el DataFrame en una lista de tuplas para insertar los datos\n",
    "    datos = [tuple(row) for row in df.collect()]\n",
    "    columnas = \", \".join(df.columns)  # Nombres de las columnas separados por comas\n",
    "    placeholders = \", \".join([\"%s\"] * len(df.columns))  # Placeholder para cada columna\n",
    "    sql_insertar = f\"INSERT INTO {nombre_tabla} ({columnas}) VALUES ({placeholders})\"\n",
    "    \n",
    "    try:\n",
    "        # Conexi√≥n a PostgreSQL e inserci√≥n de datos\n",
    "        with psycopg.connect(**conn_params) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.executemany(sql_insertar, datos)\n",
    "            conn.commit()\n",
    "        print(f\"Datos insertados correctamente en la tabla '{nombre_tabla}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar datos en la tabla '{nombre_tabla}': {e}\")\n",
    "\n",
    "# ================================================\n",
    "# Ajustar nombres de columnas\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque ajusta los nombres de columnas de los DataFrames `df_oij` y `df_inec` \n",
    "# para que sean compatibles con PostgreSQL y estandarizados.\n",
    "\n",
    "df_oij = df_oij.toDF(*[\n",
    "    \"delito\", \"subdelito\", \"fecha\", \"hora\", \"victima\", \"subvictima\",\n",
    "    \"edad\", \"sexo\", \"nacionalidad\", \"provincia\", \"canton\", \"distrito\"\n",
    "])  # Renombrar columnas para `df_oij`\n",
    "\n",
    "df_inec = df_inec.toDF(\n",
    "    \"provincia\", \"canton\", \"distrito\", \"poblacion_mayor_a_15\",\n",
    "    \"tasa_neta_de_participacion\", \"tasa_de_ocupacion\",\n",
    "    \"tasa_de_desempleo_abierto\", \"porcentaje_de_poblacion_economicamente_inactiva\",\n",
    "    \"relacion_de_dependencia_economica\", \"porcentaje_poblacion_sector_primario\",\n",
    "    \"porcentaje_poblacion_sector_secundario\", \"porcentaje_poblacion_sector_terciario\"\n",
    ")  # Renombrar columnas para `df_inec`\n",
    "\n",
    "# ================================================\n",
    "# Crear tablas e insertar datos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque automatiza la creaci√≥n de tablas y la inserci√≥n de datos en PostgreSQL.\n",
    "# Se ajustan los nombres de columnas para evitar errores relacionados con espacios o may√∫sculas.\n",
    "\n",
    "tablas = [\n",
    "    (\"oij\", df_oij),  # Tabla para los datos de OIJ\n",
    "    (\"inec\", df_inec)  # Tabla para los datos de INEC\n",
    "]\n",
    "\n",
    "for nombre_tabla, dataframe in tablas:\n",
    "    try:\n",
    "        # Ajustar nombres de columnas (sin espacios y en min√∫sculas)\n",
    "        dataframe = dataframe.toDF(*[col.replace(\" \", \"_\").lower() for col in dataframe.columns])\n",
    "\n",
    "        # Crear la tabla en PostgreSQL\n",
    "        sql_crear_tabla = crear_tabla_sql(dataframe, nombre_tabla)\n",
    "        with psycopg.connect(**conn_params) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(sql_crear_tabla)\n",
    "            conn.commit()\n",
    "        print(f\"Tabla '{nombre_tabla}' creada correctamente.\")\n",
    "\n",
    "        # Insertar los datos en la tabla\n",
    "        insertar_datos_postgres(dataframe, nombre_tabla, conn_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la tabla '{nombre_tabla}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar Tildes de la Columna distritos de inec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "# ================================================\n",
    "# Eliminar tildes de la columna 'distrito'\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque utiliza la funci√≥n `translate` de PySpark para eliminar las tildes\n",
    "# de los valores en la columna `distrito` del DataFrame `df_inec`.\n",
    "\n",
    "# Par√°metros:\n",
    "# - translate(\"columna\", \"caracteres_a_reemplazar\", \"caracteres_de_reemplazo\"):\n",
    "#   - \"distrito\": Especifica la columna que ser√° transformada.\n",
    "#   - \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\": Lista de caracteres con tildes que ser√°n reemplazados.\n",
    "#   - \"aeiouAEIOU\": Lista de caracteres sin tildes que reemplazar√°n a los anteriores.\n",
    "\n",
    "# Resultado:\n",
    "# - Crea una nueva versi√≥n de la columna `distrito` en el DataFrame `df_inec`\n",
    "#   con los valores transformados (sin tildes).\n",
    "\n",
    "# Definir columnas a procesar\n",
    "columnas_inec = [\"Provincia\", \"Canton\", \"distrito\"]\n",
    "columnas_oij = [\"Provincia\", \"Canton\", \"Distrito\"]\n",
    "\n",
    "# Eliminar tildes en cada columna de df_inec\n",
    "for columna in columnas_inec:\n",
    "    df_inec = df_inec.withColumn(\n",
    "        columna,\n",
    "        translate(columna, \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\", \"aeiouAEIOU\")\n",
    "    )\n",
    "\n",
    "# Eliminar tildes en cada columna de df_oij\n",
    "for columna in columnas_oij:\n",
    "    df_oij = df_oij.withColumn(\n",
    "        columna,\n",
    "        translate(columna, \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\", \"aeiouAEIOU\")\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# Mostrar resultados\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque utiliza el m√©todo `select` para mostrar √∫nicamente la columna `distrito`\n",
    "# despu√©s de la transformaci√≥n, permitiendo verificar que las tildes hayan sido eliminadas.\n",
    "\n",
    "\n",
    "df_inec.select(\"Provincia\", \"Canton\", \"distrito\").show(truncate=False)\n",
    "df_oij.select(\"Provincia\", \"Canton\", \"Distrito\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar tildes en la base de datos (columna distritos, tabla inec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Eliminaci√≥n de tildes en la tabla 'inec'\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque ejecuta una consulta SQL para actualizar los valores de la columna `distrito`\n",
    "# en la tabla `inec` de PostgreSQL, eliminando las tildes utilizando la funci√≥n `translate`.\n",
    "\n",
    "# Consulta SQL para la tabla 'inec'\n",
    "sql_update_inec = \"\"\"\n",
    "UPDATE inec\n",
    "SET \n",
    "    Provincia = translate(Provincia, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU'),\n",
    "    Canton = translate(Canton, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU'),\n",
    "    distrito = translate(distrito, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU');\n",
    "\"\"\"\n",
    "\n",
    "# Consulta SQL para la tabla 'oij'\n",
    "sql_update_oij = \"\"\"\n",
    "UPDATE oij\n",
    "SET \n",
    "    Provincia = translate(Provincia, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU'),\n",
    "    Canton = translate(Canton, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU'),\n",
    "    Distrito = translate(Distrito, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU');\n",
    "\"\"\"\n",
    "\n",
    "# Descripci√≥n de la consulta:\n",
    "# - `UPDATE inec`: Actualiza los registros de la tabla `inec`.\n",
    "# - `SET distrito`: Especifica que se modificar√° la columna `distrito`.\n",
    "# - `translate(distrito, '√°√©√≠√≥√∫√Å√â√ç√ì√ö', 'aeiouAEIOU')`: Reemplaza cada vocal con tilde\n",
    "#   por su equivalente sin tilde directamente en la base de datos.\n",
    "\n",
    "# Conexi√≥n a PostgreSQL y ejecuci√≥n de la consulta\n",
    "try:\n",
    "    with psycopg.connect(**conn_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Actualizar tabla 'inec'\n",
    "            cur.execute(sql_update_inec)\n",
    "            # Actualizar tabla 'oij'\n",
    "            cur.execute(sql_update_oij)\n",
    "        conn.commit()\n",
    "    print(\"Tildes eliminadas en las tablas 'inec' y 'oij'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al eliminar tildes en las tablas: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Una funci√≥n que devuelva la lista de distritos del conjunto de datos del OIJ que no coinciden con ning√∫n distrito del conjunto de datos del INEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, translate\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: obtener_distritos_no_coincidentes\n",
    "# ================================================\n",
    "def obtener_distritos_no_coincidentes(oij_df, inec_df, columna_oij, columna_inec):\n",
    "    \"\"\"\n",
    "    Devuelve la lista de distritos del conjunto de datos del OIJ que no coinciden\n",
    "    con ning√∫n distrito del conjunto de datos del INEC.\n",
    "\n",
    "    Args:\n",
    "        oij_df: DataFrame de PySpark con los datos del OIJ.\n",
    "        inec_df: DataFrame de PySpark con los datos del INEC.\n",
    "        columna_oij: Nombre de la columna de distrito en el DataFrame OIJ.\n",
    "        columna_inec: Nombre de la columna de distrito en el DataFrame INEC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Un DataFrame con los distritos del OIJ que no coinciden con los del INEC.\n",
    "\n",
    "    Descripci√≥n de bloques relevantes:\n",
    "    1. **Limpieza de tildes**:\n",
    "        Se utiliza la funci√≥n `translate` para eliminar las tildes de ambas columnas\n",
    "        (`columna_oij` y `columna_inec`) en los DataFrames del OIJ y el INEC, respectivamente.\n",
    "    2. **Left Anti Join**:\n",
    "        Se realiza un \"left anti join\" entre los dos DataFrames para identificar los distritos\n",
    "        en el OIJ que no tienen coincidencia en el INEC.\n",
    "    3. **Selecci√≥n de columna**:\n",
    "        Se selecciona √∫nicamente la columna de distrito del OIJ para devolver un DataFrame\n",
    "        con valores √∫nicos.\n",
    "    \"\"\"\n",
    "    # Limpiar tildes en ambas columnas para asegurar comparaciones consistentes\n",
    "    oij_df = oij_df.withColumn(\n",
    "        columna_oij,\n",
    "        translate(col(columna_oij), \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\", \"aeiouAEIOU\")\n",
    "    )\n",
    "    inec_df = inec_df.withColumn(\n",
    "        columna_inec,\n",
    "        translate(col(columna_inec), \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\", \"aeiouAEIOU\")\n",
    "    )\n",
    "\n",
    "    # Realizar el left anti join\n",
    "    distritos_no_coincidentes = oij_df.join(\n",
    "        inec_df,\n",
    "        oij_df[columna_oij] == inec_df[columna_inec],\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    # Seleccionar √∫nicamente la columna de distritos del OIJ\n",
    "    return distritos_no_coincidentes.select(columna_oij).distinct()\n",
    "\n",
    "# ================================================\n",
    "# Uso de la funci√≥n\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque aplica la funci√≥n `obtener_distritos_no_coincidentes` para encontrar\n",
    "# los distritos en el DataFrame del OIJ que no tienen coincidencia en el INEC.\n",
    "\n",
    "# Par√°metros:\n",
    "# - `oij_df`: DataFrame de Spark con los datos del OIJ.\n",
    "# - `inec_df`: DataFrame de Spark con los datos del INEC.\n",
    "# - `columna_oij`: Nombre de la columna de distrito en el DataFrame OIJ.\n",
    "# - `columna_inec`: Nombre de la columna de distrito en el DataFrame INEC.\n",
    "\n",
    "distritos_no_coincidentes = obtener_distritos_no_coincidentes(\n",
    "    oij_df=df_oij,\n",
    "    inec_df=df_inec,\n",
    "    columna_oij=\"distrito\",\n",
    "    columna_inec=\"distrito\"\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Mostrar los distritos no coincidentes\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque utiliza `show()` para visualizar los distritos del OIJ que no tienen coincidencia\n",
    "# en el DataFrame del INEC. El argumento `truncate=False` asegura que los valores largos\n",
    "# no sean truncados.\n",
    "\n",
    "distritos_no_coincidentes.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Una funci√≥n que devuelva la cantidad de registros en el conjunto de datos del OIJ que no coinciden con ning√∫n distrito del conjunto de datos del INEC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, lower, col, translate\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: contar_distritos_no_coincidentes\n",
    "# ================================================\n",
    "def contar_distritos_no_coincidentes(oij_df, inec_df, columna_oij=\"Distrito\", columna_inec=\"distrito\"):\n",
    "    \"\"\"\n",
    "    Cuenta la cantidad de registros en el conjunto de datos del OIJ\n",
    "    que no coinciden con ning√∫n distrito del conjunto de datos del INEC.\n",
    "\n",
    "    Args:\n",
    "        oij_df (DataFrame): DataFrame del conjunto de datos del OIJ.\n",
    "        inec_df (DataFrame): DataFrame del conjunto de datos del INEC.\n",
    "        columna_oij (str): Nombre de la columna de distrito en el DataFrame OIJ.\n",
    "        Por defecto, \"Distrito\".\n",
    "        columna_inec (str): Nombre de la columna de distrito en el DataFrame INEC.\n",
    "        Por defecto, \"distrito\".\n",
    "\n",
    "    Returns:\n",
    "        int: N√∫mero de registros del OIJ sin coincidencia en INEC.\n",
    "\n",
    "    Descripci√≥n de bloques relevantes:\n",
    "    1. **Normalizaci√≥n de columnas**:\n",
    "        - Se eliminan espacios en blanco con `trim`.\n",
    "        - Se convierten los valores a min√∫sculas con `lower`.\n",
    "        - Se eliminan tildes con `translate` para asegurar una comparaci√≥n consistente.\n",
    "    2. **Left Anti Join**:\n",
    "        - Identifica los distritos en el DataFrame del OIJ que no tienen coincidencia en el INEC.\n",
    "    3. **Conteo de resultados**:\n",
    "        - Cuenta los registros no coincidentes utilizando `count()`.\n",
    "    \"\"\"\n",
    "    # Normalizar la columna de distrito en OIJ eliminando tildes, espacios y pasando a min√∫sculas\n",
    "    oij_df = oij_df.withColumn(\n",
    "        columna_oij,\n",
    "        translate(trim(lower(col(columna_oij))), \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\", \"aeiouAEIOU\")\n",
    "    )\n",
    "    \n",
    "    # Normalizar la columna de distrito en INEC eliminando tildes, espacios y pasando a min√∫sculas\n",
    "    inec_df = inec_df.withColumn(\n",
    "        columna_inec,\n",
    "        translate(trim(lower(col(columna_inec))), \"√°√©√≠√≥√∫√Å√â√ç√ì√ö\", \"aeiouAEIOU\")\n",
    "    )\n",
    "    \n",
    "    # Realizar un left anti join para encontrar los distritos del OIJ que no tienen coincidencia en INEC\n",
    "    distritos_no_coincidentes = oij_df.join(\n",
    "        inec_df,\n",
    "        oij_df[columna_oij] == inec_df[columna_inec],\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    \n",
    "    # Contar y devolver el n√∫mero de registros no coincidentes\n",
    "    cantidad_no_coincidentes = distritos_no_coincidentes.count()\n",
    "    return cantidad_no_coincidentes\n",
    "\n",
    "# ================================================\n",
    "# Crear una sesi√≥n de Spark\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque inicializa una sesi√≥n de Spark para realizar operaciones en los DataFrames.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Comparar Distritos OIJ e INEC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ================================================\n",
    "# Cargar datasets desde archivos CSV\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque (actualmente comentado) permite cargar los datasets OIJ e INEC desde archivos CSV\n",
    "# y convertirlos en DataFrames de Spark.\n",
    "\n",
    "# df_oij = spark.read.csv(\"C:\\\\Projects\\\\spark\\\\data\\\\OIJ2011.csv\", header=True, inferSchema=True)\n",
    "# df_inec = spark.read.csv(\"C:\\\\Projects\\\\spark\\\\data\\\\inec.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ================================================\n",
    "# Llamar a la funci√≥n y mostrar el resultado\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque utiliza la funci√≥n `contar_distritos_no_coincidentes` para calcular\n",
    "# cu√°ntos registros del OIJ no tienen coincidencias en el INEC y muestra el resultado.\n",
    "\n",
    "resultado = contar_distritos_no_coincidentes(oij_df, inec_df)\n",
    "print(f\"N√∫mero de registros no coincidentes: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edite, utilizando SparkSQL, los nombres de los distritos del INEC para que coincidan con algunos de los del OIJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, lower, col, translate\n",
    "\n",
    "# ================================================\n",
    "# Funci√≥n: editar_distritos_inec\n",
    "# ================================================\n",
    "def editar_distritos_inec(oij_df, inec_df, columna_oij=\"Distrito\", columna_inec=\"distrito\"):\n",
    "    \"\"\"\n",
    "    Edita los nombres de los distritos en el INEC para que coincidan con algunos de los del OIJ.\n",
    "\n",
    "    Args:\n",
    "        oij_df (DataFrame): DataFrame del conjunto de datos del OIJ.\n",
    "        inec_df (DataFrame): DataFrame del conjunto de datos del INEC.\n",
    "        columna_oij (str): Nombre de la columna de distrito en el DataFrame OIJ.\n",
    "                            Por defecto, \"Distrito\".\n",
    "        columna_inec (str): Nombre de la columna de distrito en el DataFrame INEC.\n",
    "                            Por defecto, \"distrito\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataFrame, DataFrame]: DataFrames editados de OIJ e INEC.\n",
    "\n",
    "    Descripci√≥n de bloques relevantes:\n",
    "    1. **Normalizaci√≥n de columnas**:\n",
    "        - Elimina espacios en blanco con `trim`.\n",
    "        - Convierte texto a min√∫sculas con `lower`.\n",
    "        - Reemplaza caracteres con tildes y la `√±` con sus equivalentes sin tildes utilizando `translate`.\n",
    "    2. **Salida**:\n",
    "        - Retorna dos DataFrames con las columnas normalizadas.\n",
    "    \"\"\"\n",
    "    # Normalizar columnas de distritos en OIJ e INEC (quitar tildes, √± -> n, espacios, min√∫sculas)\n",
    "    oij_df = oij_df.withColumn(\n",
    "        columna_oij,\n",
    "        translate(trim(lower(col(columna_oij))), \"√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë\", \"aeiounAEIOUN\")\n",
    "    )\n",
    "    \n",
    "    inec_df = inec_df.withColumn(\n",
    "        columna_inec,\n",
    "        translate(trim(lower(col(columna_inec))), \"√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë\", \"aeiounAEIOUN\")\n",
    "    )\n",
    "    \n",
    "    return oij_df, inec_df\n",
    "\n",
    "# ================================================\n",
    "# Crear una sesi√≥n de Spark\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque inicializa una sesi√≥n de Spark necesaria para realizar operaciones\n",
    "# en los DataFrames y cargar los datos.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Editar Distritos INEC para Coincidir con OIJ\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ================================================\n",
    "# Cargar datasets desde archivos CSV\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque (comentado por defecto) carga los datasets desde archivos CSV y los convierte en DataFrames de Spark.\n",
    "\n",
    "# df_oij = spark.read.csv(\"C:\\\\Projects\\\\spark\\\\data\\\\OIJ2011.csv\", header=True, inferSchema=True)\n",
    "# df_inec = spark.read.csv(\"C:\\\\Projects\\\\spark\\\\data\\\\inec.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ================================================\n",
    "# Llamar a la funci√≥n para editar distritos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Se aplica la funci√≥n `editar_distritos_inec` para normalizar las columnas de distritos\n",
    "# en los DataFrames del OIJ e INEC.\n",
    "\n",
    "df_oij_editado, df_inec_editado = editar_distritos_inec(df_oij, df_inec)\n",
    "\n",
    "# ================================================\n",
    "# Usar alias para desambiguar columnas\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Se asignan alias a los DataFrames editados para facilitar la referencia a sus columnas\n",
    "# durante las operaciones de uni√≥n y filtrado.\n",
    "\n",
    "df_oij_editado = df_oij_editado.alias(\"oij\")\n",
    "df_inec_editado = df_inec_editado.alias(\"inec\")\n",
    "\n",
    "# ================================================\n",
    "# Verificar coincidencias espec√≠ficas para \"ca√±as\" y \"pe√±as blancas\"\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Este bloque realiza una uni√≥n entre los DataFrames editados del OIJ e INEC y filtra los resultados\n",
    "# para buscar coincidencias espec√≠ficas que incluyan los t√©rminos \"ca√±as\" y \"pe√±as blancas\".\n",
    "\n",
    "coincidencias_especificas = df_oij_editado.join(\n",
    "    df_inec_editado,\n",
    "    col(\"oij.Distrito\") == col(\"inec.distrito\"),  # Unir por distritos normalizados\n",
    "    \"inner\"\n",
    ").filter(\n",
    "    (col(\"oij.Distrito\").like(\"%canas%\")) |  # Buscar coincidencias con \"ca√±as\"\n",
    "    (col(\"oij.Distrito\").like(\"%penas blancas%\"))  # Buscar coincidencias con \"pe√±as blancas\"\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Coincidencias espec√≠ficas para 'ca√±as' y 'pe√±as blancas':\")\n",
    "coincidencias_especificas.select(\"oij.Distrito\", \"inec.distrito\").distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZACI√ìN DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de Delitos y Tasa de Ocupaci√≥n para los 10 Distritos con M√°s Delitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "# ================================================\n",
    "# Obtener la tasa de ocupaci√≥n de los distritos correspondientes\n",
    "# ================================================\n",
    "tasa_ocupacion = (\n",
    "    df_inec.join(\n",
    "        top_distritos,\n",
    "        on=[\"provincia\", \"canton\", \"distrito\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .select(\"provincia\", \"canton\", \"distrito\", \"tasa_de_ocupacion\")\n",
    "    .toPandas()\n",
    ")\n",
    "tasa_ocupacion[\"region\"] = tasa_ocupacion[\"provincia\"] + \" - \" + tasa_ocupacion[\"canton\"] + \" - \" + tasa_ocupacion[\"distrito\"]\n",
    "\n",
    "# ================================================\n",
    "# Combinar los datos\n",
    "# ================================================\n",
    "merged_data = pd.merge(\n",
    "    top_distritos_pd,\n",
    "    tasa_ocupacion,\n",
    "    on=\"region\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rellenar valores faltantes de tasa de ocupaci√≥n con un valor predeterminado (por ejemplo, 0)\n",
    "merged_data[\"tasa_de_ocupacion\"].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# Graficar los datos\n",
    "# ================================================\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gr√°fico de barras para la cantidad de delitos\n",
    "plt.bar(\n",
    "    merged_data[\"region\"],\n",
    "    merged_data[\"count\"],\n",
    "    label=\"Cantidad de Delitos\",\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Gr√°fico de l√≠neas para la tasa de ocupaci√≥n\n",
    "plt.plot(\n",
    "    merged_data[\"region\"],\n",
    "    merged_data[\"tasa_de_ocupacion\"],\n",
    "    label=\"Tasa de Ocupaci√≥n (%)\",\n",
    "    color=\"orange\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\"\n",
    ")\n",
    "\n",
    "plt.title(\"Cantidad de Delitos y Tasa de Ocupaci√≥n para los 10 Distritos con M√°s Delitos\")\n",
    "plt.xlabel(\"Distrito\")\n",
    "plt.ylabel(\"Cantidad/Tasa (%)\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graficar con eje secundario\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax1.bar(\n",
    "    merged_data[\"region\"],\n",
    "    merged_data[\"count\"],\n",
    "    label=\"Cantidad de Delitos\",\n",
    "    alpha=0.7,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "ax1.set_xlabel(\"Distrito, Cant√≥n y Provincia\")\n",
    "ax1.set_ylabel(\"Cantidad de Delitos\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "ax1.set_title(\"Cantidad de Delitos y Tasa de Ocupaci√≥n para los 10 Distritos con M√°s Delitos\")\n",
    "ax1.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    merged_data[\"region\"],\n",
    "    merged_data[\"tasa_de_ocupacion\"],\n",
    "    label=\"Tasa de Ocupaci√≥n (%)\",\n",
    "    color=\"orange\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax2.set_ylabel(\"Tasa de Ocupaci√≥n (%)\", color=\"orange\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"orange\")\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de Delitos por D√≠a de la Semana "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import dayofweek, to_date, col\n",
    "import warnings\n",
    "\n",
    "# ================================================\n",
    "# Suprimir FutureWarnings de seaborn\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Se deshabilitan las advertencias futuras para evitar que interfieran con la visualizaci√≥n del gr√°fico.\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ================================================\n",
    "# Asegurar que la columna 'Fecha' est√© en formato de fecha\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Convierte la columna `Fecha` en el DataFrame editado del OIJ (`df_oij_editado`)\n",
    "# al formato de fecha utilizando `to_date`.\n",
    "\n",
    "df_oij_editado = df_oij_editado.withColumn(\n",
    "    \"Fecha\", \n",
    "    to_date(col(\"Fecha\"), \"M/d/yyyy\")  # Define el formato de entrada de la fecha\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Encontrar el distrito con m√°s delitos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Agrupa los datos por la columna `Distrito`, cuenta la cantidad de registros,\n",
    "# y selecciona el distrito con la mayor cantidad de delitos.\n",
    "\n",
    "distrito_mas_delitos = df_oij_editado.groupBy(\"Distrito\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .first()[\"Distrito\"]\n",
    "\n",
    "# ================================================\n",
    "# Filtrar datos para el distrito con m√°s delitos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Filtra el DataFrame para incluir √∫nicamente los registros correspondientes\n",
    "# al distrito con m√°s delitos.\n",
    "\n",
    "delitos_distrito = df_oij_editado.filter(\n",
    "    col(\"Distrito\") == distrito_mas_delitos\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Extraer el d√≠a de la semana\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# A√±ade una nueva columna `DiaSemana` que contiene el n√∫mero del d√≠a de la semana\n",
    "# (1 = Domingo, 7 = S√°bado) basado en la columna `Fecha`.\n",
    "\n",
    "delitos_distrito = delitos_distrito.withColumn(\n",
    "    \"DiaSemana\", \n",
    "    dayofweek(col(\"Fecha\"))  # Calcula el d√≠a de la semana\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Mapear los n√∫meros de d√≠a a nombres\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Define un diccionario para mapear los n√∫meros de los d√≠as de la semana\n",
    "# a sus nombres correspondientes.\n",
    "\n",
    "dias_mapping = {\n",
    "    1: \"Domingo\",\n",
    "    2: \"Lunes\",\n",
    "    3: \"Martes\",\n",
    "    4: \"Mi√©rcoles\",\n",
    "    5: \"Jueves\",\n",
    "    6: \"Viernes\",\n",
    "    7: \"S√°bado\"\n",
    "}\n",
    "\n",
    "# Convertir a Pandas DataFrame y mapear los d√≠as\n",
    "delitos_distrito_pd = delitos_distrito.toPandas()\n",
    "delitos_distrito_pd['DiaSemana'] = delitos_distrito_pd['DiaSemana'].map(dias_mapping)\n",
    "\n",
    "# ================================================\n",
    "# Contar delitos por d√≠a de la semana\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Cuenta el n√∫mero de delitos por cada d√≠a de la semana y asegura\n",
    "# que los d√≠as aparezcan en el orden correcto.\n",
    "\n",
    "conteo_dias = delitos_distrito_pd['DiaSemana'].value_counts().reindex([\n",
    "    \"Domingo\", \"Lunes\", \"Martes\", \"Mi√©rcoles\", \"Jueves\", \"Viernes\", \"S√°bado\"\n",
    "]).fillna(0)\n",
    "\n",
    "# ================================================\n",
    "# Gr√°fico de barras\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Crea un gr√°fico de barras para mostrar la cantidad de delitos por d√≠a de la semana\n",
    "# en el distrito con m√°s delitos.\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Tama√±o del gr√°fico\n",
    "sns.barplot(\n",
    "    x=conteo_dias.index,  # Eje X: D√≠as de la semana\n",
    "    y=conteo_dias.values,  # Eje Y: Cantidad de delitos\n",
    "    palette=sns.color_palette(\"magma\", len(conteo_dias.index))  # Paleta de colores\n",
    ")\n",
    "\n",
    "# Configurar t√≠tulo y etiquetas\n",
    "plt.title(f\"Cantidad de Delitos por D√≠a de la Semana en {distrito_mas_delitos.capitalize()}\")\n",
    "plt.xlabel(\"D√≠a de la Semana\")\n",
    "plt.ylabel(\"Cantidad de Delitos\")\n",
    "plt.xticks(rotation=45)  # Rotar etiquetas del eje X\n",
    "plt.tight_layout()  # Ajustar dise√±o para evitar solapamientos\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de Delitos por Tipo en el Distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================================================\n",
    "# Selecci√≥n de un distrito\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Define el distrito, cant√≥n y provincia que se desean analizar. Puedes cambiar\n",
    "# las variables `distrito_seleccionado`, `canton_seleccionado` y `provincia_seleccionada`\n",
    "# para explorar otros lugares.\n",
    "\n",
    "distrito_seleccionado = \"carmen\"  # Cambiar al distrito deseado\n",
    "canton_seleccionado = \"sanjose\"  # Cambiar al cant√≥n deseado\n",
    "provincia_seleccionada = \"sanjose\"  # Cambiar a la provincia deseada\n",
    "\n",
    "# ================================================\n",
    "# Filtrar los datos para el distrito seleccionado\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Filtra los registros en el DataFrame `df_oij` para incluir √∫nicamente aquellos\n",
    "# que coincidan con el distrito, cant√≥n y provincia seleccionados. Luego, agrupa\n",
    "# los datos por tipo de delito y cuenta la cantidad de registros en cada grupo.\n",
    "\n",
    "delitos_por_tipo_distrito = (\n",
    "    df_oij.filter(  # Filtrar por distrito, cant√≥n y provincia\n",
    "        (col(\"distrito\") == distrito_seleccionado) &  # Condici√≥n para el distrito\n",
    "        (col(\"Canton\") == canton_seleccionado) &  # Condici√≥n para el cant√≥n\n",
    "        (col(\"Provincia\") == provincia_seleccionada)  # Condici√≥n para la provincia\n",
    "    )\n",
    "    .groupBy(\"delito\")  # Agrupar por tipo de delito\n",
    "    .count()  # Contar registros en cada grupo\n",
    "    .orderBy(\"count\", ascending=False)  # Ordenar por cantidad de delitos (descendente)\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Verificar si se encontraron resultados\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Comprueba si el DataFrame contiene datos para el distrito seleccionado. Si no\n",
    "# hay datos, imprime un mensaje indicando que no se encontr√≥ el distrito.\n",
    "\n",
    "if delitos_por_tipo_distrito.count() > 0:  # Si hay registros en el DataFrame...\n",
    "    \n",
    "    # ================================================\n",
    "    # Convertir a Pandas DataFrame\n",
    "    # ================================================\n",
    "    # Descripci√≥n general:\n",
    "    # Convierte el resultado del DataFrame de PySpark a un DataFrame de Pandas\n",
    "    # para facilitar la manipulaci√≥n y la creaci√≥n de gr√°ficos.\n",
    "    \n",
    "    delitos_por_tipo_distrito_pd = delitos_por_tipo_distrito.toPandas()\n",
    "\n",
    "    # ================================================\n",
    "    # Graficar los datos\n",
    "    # ================================================\n",
    "    # Descripci√≥n general:\n",
    "    # Crea un gr√°fico de barras horizontales para visualizar la cantidad de delitos\n",
    "    # por tipo en el distrito seleccionado.\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))  # Establecer el tama√±o del gr√°fico\n",
    "    plt.barh(\n",
    "        delitos_por_tipo_distrito_pd[\"delito\"],  # Eje Y: Tipos de delitos\n",
    "        delitos_por_tipo_distrito_pd[\"count\"],  # Eje X: Cantidad de delitos\n",
    "        color=\"skyblue\"  # Color de las barras\n",
    "    )\n",
    "\n",
    "    # Configuraci√≥n del t√≠tulo y etiquetas\n",
    "    plt.title(f\"Cantidad de Delitos por Tipo en el Distrito: {distrito_seleccionado.title()}\")\n",
    "    plt.xlabel(\"Cantidad de Delitos\")  # Etiqueta del eje X\n",
    "    plt.ylabel(\"Tipo de Delito\")  # Etiqueta del eje Y\n",
    "    plt.tight_layout()  # Ajustar dise√±o para evitar solapamientos\n",
    "    plt.show()  # Mostrar el gr√°fico\n",
    "\n",
    "else:\n",
    "    # ================================================\n",
    "    # Mensaje si no se encontraron resultados\n",
    "    # ================================================\n",
    "    # Descripci√≥n general:\n",
    "    # Si el DataFrame est√° vac√≠o, imprime un mensaje indicando que no se encontr√≥\n",
    "    # el distrito especificado en los datos.\n",
    "\n",
    "    print(\"No se encontr√≥ el distrito\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de Delitos por Sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================================================\n",
    "# Agrupar por sexo y contar la cantidad de delitos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Agrupa los datos en el DataFrame `df_oij` por la columna `sexo`, cuenta el\n",
    "# n√∫mero de registros por cada categor√≠a, y ordena los resultados en orden\n",
    "# descendente seg√∫n la cantidad.\n",
    "\n",
    "delitos_por_sexo = df_oij.groupBy(\"sexo\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)  # Ordenar por cantidad de delitos\n",
    "\n",
    "# ================================================\n",
    "# Convertir a Pandas DataFrame\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Convierte el resultado del DataFrame de PySpark a un DataFrame de Pandas para\n",
    "# facilitar la manipulaci√≥n y la creaci√≥n de gr√°ficos.\n",
    "\n",
    "delitos_por_sexo_pd = delitos_por_sexo.toPandas()\n",
    "\n",
    "# ================================================\n",
    "# Graficar\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Crea un gr√°fico de barras para visualizar la cantidad de delitos por sexo.\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Establecer el tama√±o del gr√°fico\n",
    "plt.bar(\n",
    "    delitos_por_sexo_pd[\"sexo\"],  # Eje X: Categor√≠as de sexo\n",
    "    delitos_por_sexo_pd[\"count\"],  # Eje Y: Cantidad de delitos\n",
    "    color=[\"blue\", \"pink\"]  # Colores para las barras\n",
    ")\n",
    "\n",
    "# Configuraci√≥n del t√≠tulo y etiquetas\n",
    "plt.title(\"Cantidad de Delitos por Sexo\")\n",
    "plt.xlabel(\"Sexo\")\n",
    "plt.ylabel(\"Cantidad de Delitos\")\n",
    "plt.xticks(rotation=0)  # Asegurar que las etiquetas del eje X est√©n horizontalmente alineadas\n",
    "plt.tight_layout()  # Ajustar dise√±o para evitar solapamiento\n",
    "plt.show()  # Mostrar el gr√°fico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propuesta de Visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ================================================\n",
    "# Extraer la hora de inicio\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Agrega una nueva columna `hora_inicio` al DataFrame `df_oij` que contiene\n",
    "# √∫nicamente las horas (los dos primeros caracteres) de la columna `hora`.\n",
    "\n",
    "df_oij_with_hour = df_oij.withColumn(\n",
    "    \"hora_inicio\", \n",
    "    col(\"hora\").substr(1, 2)  # Extraer los dos primeros caracteres de la columna `hora`\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# Agrupar los datos por hora del d√≠a y tipo de delito\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Agrupa los datos del DataFrame por las columnas `hora_inicio` y `delito`,\n",
    "# contando la cantidad de registros para cada combinaci√≥n.\n",
    "\n",
    "delitos_por_hora_y_tipo = df_oij_with_hour.groupBy(\"hora_inicio\", \"delito\") \\\n",
    "    .count()  # Contar registros en cada grupo\n",
    "\n",
    "# ================================================\n",
    "# Convertir a Pandas para graficar\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Convierte el resultado del DataFrame de PySpark a un DataFrame de Pandas\n",
    "# para poder graficarlo utilizando Matplotlib y Seaborn.\n",
    "\n",
    "delitos_por_hora_y_tipo_pd = delitos_por_hora_y_tipo.toPandas()\n",
    "\n",
    "# ================================================\n",
    "# Pivoteo de datos\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Reorganiza los datos para que las filas correspondan a las horas (`hora_inicio`),\n",
    "# las columnas correspondan a los tipos de delitos (`delito`), y los valores\n",
    "# correspondan a la cantidad de delitos.\n",
    "\n",
    "heatmap_data = delitos_por_hora_y_tipo_pd.pivot(\n",
    "    index=\"hora_inicio\",  # Filas: Hora del d√≠a\n",
    "    columns=\"delito\",  # Columnas: Tipos de delitos\n",
    "    values=\"count\"  # Valores: Cantidad de delitos\n",
    ").fillna(0)  # Rellenar valores faltantes con 0\n",
    "\n",
    "# ================================================\n",
    "# Crear el mapa de calor\n",
    "# ================================================\n",
    "# Descripci√≥n general:\n",
    "# Crea un mapa de calor utilizando Seaborn para visualizar la cantidad de delitos\n",
    "# por tipo y hora del d√≠a.\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Establecer el tama√±o del gr√°fico\n",
    "sns.heatmap(\n",
    "    heatmap_data,  # Datos para el mapa de calor\n",
    "    cmap=\"YlOrRd\",  # Esquema de colores (amarillo a rojo)\n",
    "    annot=True,  # Mostrar los valores en las celdas\n",
    "    fmt=\".0f\",  # Formato de los valores (n√∫meros enteros)\n",
    "    linewidths=0.5,  # L√≠neas entre celdas\n",
    "    cbar_kws={'label': 'Cantidad de Delitos'}  # Etiqueta para la barra de colores\n",
    ")\n",
    "\n",
    "# Personalizaci√≥n del gr√°fico\n",
    "plt.title(\"Mapa de Calor: Delitos por Tipo y Hora del D√≠a\", fontsize=14)  # T√≠tulo\n",
    "plt.xlabel(\"Tipo de Delito\", fontsize=12)  # Etiqueta del eje X\n",
    "plt.ylabel(\"Hora del D√≠a\", fontsize=12)  # Etiqueta del eje Y\n",
    "plt.xticks(rotation=45)  # Rotar etiquetas del eje X para mejor visibilidad\n",
    "plt.tight_layout()  # Ajustar dise√±o para evitar solapamiento\n",
    "\n",
    "# Mostrar el gr√°fico\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
